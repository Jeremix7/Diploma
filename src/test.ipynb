{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "912d93de",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "your 131072x1 screen size is bogus. expect trouble\n",
      "25/05/06 17:28:03 WARN Utils: Your hostname, R2-D2 resolves to a loopback address: 127.0.1.1; using 172.21.73.132 instead (on interface eth0)\n",
      "25/05/06 17:28:03 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "25/05/06 17:28:04 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "25/05/06 17:28:05 WARN DependencyUtils: Local jar /path/to/jdbc_driver.jar does not exist, skipping.\n",
      "25/05/06 17:28:05 INFO SparkContext: Running Spark version 3.5.4\n",
      "25/05/06 17:28:05 INFO SparkContext: OS info Linux, 5.15.167.4-microsoft-standard-WSL2, amd64\n",
      "25/05/06 17:28:05 INFO SparkContext: Java version 11.0.26\n",
      "25/05/06 17:28:05 INFO ResourceUtils: ==============================================================\n",
      "25/05/06 17:28:05 INFO ResourceUtils: No custom resources configured for spark.driver.\n",
      "25/05/06 17:28:05 INFO ResourceUtils: ==============================================================\n",
      "25/05/06 17:28:05 INFO SparkContext: Submitted application: pyspark-shell\n",
      "25/05/06 17:28:05 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 8192, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)\n",
      "25/05/06 17:28:05 INFO ResourceProfile: Limiting resource is cpu\n",
      "25/05/06 17:28:05 INFO ResourceProfileManager: Added ResourceProfile id: 0\n",
      "25/05/06 17:28:05 INFO SecurityManager: Changing view acls to: jeremix\n",
      "25/05/06 17:28:05 INFO SecurityManager: Changing modify acls to: jeremix\n",
      "25/05/06 17:28:05 INFO SecurityManager: Changing view acls groups to: \n",
      "25/05/06 17:28:05 INFO SecurityManager: Changing modify acls groups to: \n",
      "25/05/06 17:28:05 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: jeremix; groups with view permissions: EMPTY; users with modify permissions: jeremix; groups with modify permissions: EMPTY\n",
      "25/05/06 17:28:05 INFO Utils: Successfully started service 'sparkDriver' on port 40145.\n",
      "25/05/06 17:28:05 INFO SparkEnv: Registering MapOutputTracker\n",
      "25/05/06 17:28:05 INFO SparkEnv: Registering BlockManagerMaster\n",
      "25/05/06 17:28:05 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
      "25/05/06 17:28:05 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n",
      "25/05/06 17:28:05 INFO SparkEnv: Registering BlockManagerMasterHeartbeat\n",
      "25/05/06 17:28:05 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-da7f4999-6370-4ba0-a7a2-dbbd5c214f3f\n",
      "25/05/06 17:28:05 INFO MemoryStore: MemoryStore started with capacity 2.2 GiB\n",
      "25/05/06 17:28:05 INFO SparkEnv: Registering OutputCommitCoordinator\n",
      "25/05/06 17:28:05 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI\n",
      "25/05/06 17:28:05 INFO Utils: Successfully started service 'SparkUI' on port 4040.\n",
      "25/05/06 17:28:05 ERROR SparkContext: Failed to add /path/to/jdbc_driver.jar to Spark environment\n",
      "java.io.FileNotFoundException: Jar /path/to/jdbc_driver.jar not found\n",
      "\tat org.apache.spark.SparkContext.addLocalJarFile$1(SparkContext.scala:2095)\n",
      "\tat org.apache.spark.SparkContext.addJar(SparkContext.scala:2151)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$new$15(SparkContext.scala:521)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$new$15$adapted(SparkContext.scala:521)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:521)\n",
      "\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\n",
      "\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n",
      "\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:238)\n",
      "\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\n",
      "\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "25/05/06 17:28:06 INFO Executor: Starting executor ID driver on host 172.21.73.132\n",
      "25/05/06 17:28:06 INFO Executor: OS info Linux, 5.15.167.4-microsoft-standard-WSL2, amd64\n",
      "25/05/06 17:28:06 INFO Executor: Java version 11.0.26\n",
      "25/05/06 17:28:06 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''\n",
      "25/05/06 17:28:06 INFO Executor: Created or updated repl class loader org.apache.spark.util.MutableURLClassLoader@5be578e5 for default.\n",
      "25/05/06 17:28:06 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 40669.\n",
      "25/05/06 17:28:06 INFO NettyBlockTransferService: Server created on 172.21.73.132:40669\n",
      "25/05/06 17:28:06 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
      "25/05/06 17:28:06 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 172.21.73.132, 40669, None)\n",
      "25/05/06 17:28:06 INFO BlockManagerMasterEndpoint: Registering block manager 172.21.73.132:40669 with 2.2 GiB RAM, BlockManagerId(driver, 172.21.73.132, 40669, None)\n",
      "25/05/06 17:28:06 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 172.21.73.132, 40669, None)\n",
      "25/05/06 17:28:06 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 172.21.73.132, 40669, None)\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder\\\n",
    "                    .master(\"local[*]\") \\\n",
    "                    .config(\"spark.executor.memory\", \"8g\") \\\n",
    "                    .config(\"spark.driver.memory\", \"4g\") \\\n",
    "                    .config(\"spark.sql.autoBroadcastJoinThreshold\", \"-1\") \\\n",
    "                    .config(\"spark.jars\", \"/path/to/jdbc_driver.jar\") \\\n",
    "                    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf66195d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/06 17:28:08 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.\n",
      "25/05/06 17:28:08 INFO SharedState: Warehouse path is 'file:/home/diploma/Diploma/spark-warehouse'.\n",
      "25/05/06 17:28:09 INFO InMemoryFileIndex: It took 46 ms to list leaf files for 1 paths.\n",
      "25/05/06 17:28:09 INFO SparkContext: Starting job: parquet at NativeMethodAccessorImpl.java:0\n",
      "25/05/06 17:28:09 INFO DAGScheduler: Got job 0 (parquet at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/05/06 17:28:09 INFO DAGScheduler: Final stage: ResultStage 0 (parquet at NativeMethodAccessorImpl.java:0)\n",
      "25/05/06 17:28:09 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/05/06 17:28:09 INFO DAGScheduler: Missing parents: List()\n",
      "25/05/06 17:28:09 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[1] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/05/06 17:28:09 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 103.7 KiB, free 2.2 GiB)\n",
      "25/05/06 17:28:09 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 37.4 KiB, free 2.2 GiB)\n",
      "25/05/06 17:28:09 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 172.21.73.132:40669 (size: 37.4 KiB, free: 2.2 GiB)\n",
      "25/05/06 17:28:09 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1585\n",
      "25/05/06 17:28:09 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/05/06 17:28:09 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0\n",
      "25/05/06 17:28:09 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (172.21.73.132, executor driver, partition 0, PROCESS_LOCAL, 9218 bytes) \n",
      "25/05/06 17:28:09 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)\n",
      "25/05/06 17:28:10 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 2643 bytes result sent to driver\n",
      "25/05/06 17:28:10 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 371 ms on 172.21.73.132 (executor driver) (1/1)\n",
      "25/05/06 17:28:10 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool \n",
      "25/05/06 17:28:10 INFO DAGScheduler: ResultStage 0 (parquet at NativeMethodAccessorImpl.java:0) finished in 0.576 s\n",
      "25/05/06 17:28:10 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/05/06 17:28:10 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished\n",
      "25/05/06 17:28:10 INFO DAGScheduler: Job 0 finished: parquet at NativeMethodAccessorImpl.java:0, took 0.612704 s\n",
      "25/05/06 17:28:11 INFO FileSourceStrategy: Pushed Filters:                      \n",
      "25/05/06 17:28:11 INFO FileSourceStrategy: Post-Scan Filters: \n",
      "25/05/06 17:28:11 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 172.21.73.132:40669 in memory (size: 37.4 KiB, free: 2.2 GiB)\n",
      "25/05/06 17:28:11 INFO CodeGenerator: Code generated in 200.343946 ms\n",
      "25/05/06 17:28:11 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 201.6 KiB, free 2.2 GiB)\n",
      "25/05/06 17:28:11 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 34.8 KiB, free 2.2 GiB)\n",
      "25/05/06 17:28:11 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 172.21.73.132:40669 (size: 34.8 KiB, free: 2.2 GiB)\n",
      "25/05/06 17:28:11 INFO SparkContext: Created broadcast 1 from count at NativeMethodAccessorImpl.java:0\n",
      "25/05/06 17:28:11 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "25/05/06 17:28:11 INFO DAGScheduler: Registering RDD 5 (count at NativeMethodAccessorImpl.java:0) as input to shuffle 0\n",
      "25/05/06 17:28:11 INFO DAGScheduler: Got map stage job 1 (count at NativeMethodAccessorImpl.java:0) with 5 output partitions\n",
      "25/05/06 17:28:11 INFO DAGScheduler: Final stage: ShuffleMapStage 1 (count at NativeMethodAccessorImpl.java:0)\n",
      "25/05/06 17:28:11 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/05/06 17:28:11 INFO DAGScheduler: Missing parents: List()\n",
      "25/05/06 17:28:11 INFO DAGScheduler: Submitting ShuffleMapStage 1 (MapPartitionsRDD[5] at count at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/05/06 17:28:11 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 17.0 KiB, free 2.2 GiB)\n",
      "25/05/06 17:28:11 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 7.8 KiB, free 2.2 GiB)\n",
      "25/05/06 17:28:11 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 172.21.73.132:40669 (size: 7.8 KiB, free: 2.2 GiB)\n",
      "25/05/06 17:28:11 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1585\n",
      "25/05/06 17:28:11 INFO DAGScheduler: Submitting 5 missing tasks from ShuffleMapStage 1 (MapPartitionsRDD[5] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4))\n",
      "25/05/06 17:28:11 INFO TaskSchedulerImpl: Adding task set 1.0 with 5 tasks resource profile 0\n",
      "25/05/06 17:28:11 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (172.21.73.132, executor driver, partition 0, PROCESS_LOCAL, 9677 bytes) \n",
      "25/05/06 17:28:11 INFO TaskSetManager: Starting task 1.0 in stage 1.0 (TID 2) (172.21.73.132, executor driver, partition 1, PROCESS_LOCAL, 9677 bytes) \n",
      "25/05/06 17:28:11 INFO TaskSetManager: Starting task 2.0 in stage 1.0 (TID 3) (172.21.73.132, executor driver, partition 2, PROCESS_LOCAL, 9677 bytes) \n",
      "25/05/06 17:28:11 INFO TaskSetManager: Starting task 3.0 in stage 1.0 (TID 4) (172.21.73.132, executor driver, partition 3, PROCESS_LOCAL, 9677 bytes) \n",
      "25/05/06 17:28:11 INFO TaskSetManager: Starting task 4.0 in stage 1.0 (TID 5) (172.21.73.132, executor driver, partition 4, PROCESS_LOCAL, 9677 bytes) \n",
      "25/05/06 17:28:11 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)\n",
      "25/05/06 17:28:11 INFO Executor: Running task 1.0 in stage 1.0 (TID 2)\n",
      "25/05/06 17:28:11 INFO Executor: Running task 2.0 in stage 1.0 (TID 3)\n",
      "25/05/06 17:28:11 INFO Executor: Running task 3.0 in stage 1.0 (TID 4)\n",
      "25/05/06 17:28:11 INFO Executor: Running task 4.0 in stage 1.0 (TID 5)\n",
      "25/05/06 17:28:11 INFO CodeGenerator: Code generated in 22.268077 ms\n",
      "25/05/06 17:28:11 INFO FileScanRDD: Reading File path: file:///home/diploma/Diploma/cleaned_df_after_2018.parquet/part-00003-3058d851-ea6d-42cd-9be7-0680f4d2c4c9-c000.snappy.parquet, range: 0-232378, partition values: [empty row]\n",
      "25/05/06 17:28:11 INFO FileScanRDD: Reading File path: file:///home/diploma/Diploma/cleaned_df_after_2018.parquet/part-00002-3058d851-ea6d-42cd-9be7-0680f4d2c4c9-c000.snappy.parquet, range: 0-260171, partition values: [empty row]\n",
      "25/05/06 17:28:11 INFO FileScanRDD: Reading File path: file:///home/diploma/Diploma/cleaned_df_after_2018.parquet/part-00001-3058d851-ea6d-42cd-9be7-0680f4d2c4c9-c000.snappy.parquet, range: 0-270378, partition values: [empty row]\n",
      "25/05/06 17:28:11 INFO FileScanRDD: Reading File path: file:///home/diploma/Diploma/cleaned_df_after_2018.parquet/part-00004-3058d851-ea6d-42cd-9be7-0680f4d2c4c9-c000.snappy.parquet, range: 0-144037, partition values: [empty row]\n",
      "25/05/06 17:28:11 INFO FileScanRDD: Reading File path: file:///home/diploma/Diploma/cleaned_df_after_2018.parquet/part-00000-3058d851-ea6d-42cd-9be7-0680f4d2c4c9-c000.snappy.parquet, range: 0-188231, partition values: [empty row]\n",
      "25/05/06 17:28:12 INFO Executor: Finished task 1.0 in stage 1.0 (TID 2). 2180 bytes result sent to driver\n",
      "25/05/06 17:28:12 INFO Executor: Finished task 4.0 in stage 1.0 (TID 5). 2180 bytes result sent to driver\n",
      "25/05/06 17:28:12 INFO Executor: Finished task 3.0 in stage 1.0 (TID 4). 2180 bytes result sent to driver\n",
      "25/05/06 17:28:12 INFO Executor: Finished task 2.0 in stage 1.0 (TID 3). 2180 bytes result sent to driver\n",
      "25/05/06 17:28:12 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 2180 bytes result sent to driver\n",
      "25/05/06 17:28:12 INFO TaskSetManager: Finished task 1.0 in stage 1.0 (TID 2) in 277 ms on 172.21.73.132 (executor driver) (1/5)\n",
      "25/05/06 17:28:12 INFO TaskSetManager: Finished task 3.0 in stage 1.0 (TID 4) in 275 ms on 172.21.73.132 (executor driver) (2/5)\n",
      "25/05/06 17:28:12 INFO TaskSetManager: Finished task 4.0 in stage 1.0 (TID 5) in 276 ms on 172.21.73.132 (executor driver) (3/5)\n",
      "25/05/06 17:28:12 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 284 ms on 172.21.73.132 (executor driver) (4/5)\n",
      "25/05/06 17:28:12 INFO TaskSetManager: Finished task 2.0 in stage 1.0 (TID 3) in 277 ms on 172.21.73.132 (executor driver) (5/5)\n",
      "25/05/06 17:28:12 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool \n",
      "25/05/06 17:28:12 INFO DAGScheduler: ShuffleMapStage 1 (count at NativeMethodAccessorImpl.java:0) finished in 0.334 s\n",
      "25/05/06 17:28:12 INFO DAGScheduler: looking for newly runnable stages\n",
      "25/05/06 17:28:12 INFO DAGScheduler: running: Set()\n",
      "25/05/06 17:28:12 INFO DAGScheduler: waiting: Set()\n",
      "25/05/06 17:28:12 INFO DAGScheduler: failed: Set()\n",
      "25/05/06 17:28:12 INFO CodeGenerator: Code generated in 10.273583 ms\n",
      "25/05/06 17:28:12 INFO SparkContext: Starting job: count at NativeMethodAccessorImpl.java:0\n",
      "25/05/06 17:28:12 INFO DAGScheduler: Got job 2 (count at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/05/06 17:28:12 INFO DAGScheduler: Final stage: ResultStage 3 (count at NativeMethodAccessorImpl.java:0)\n",
      "25/05/06 17:28:12 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 2)\n",
      "25/05/06 17:28:12 INFO DAGScheduler: Missing parents: List()\n",
      "25/05/06 17:28:12 INFO DAGScheduler: Submitting ResultStage 3 (MapPartitionsRDD[8] at count at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/05/06 17:28:12 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 12.5 KiB, free 2.2 GiB)\n",
      "25/05/06 17:28:12 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 5.9 KiB, free 2.2 GiB)\n",
      "25/05/06 17:28:12 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 172.21.73.132:40669 (size: 5.9 KiB, free: 2.2 GiB)\n",
      "25/05/06 17:28:12 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1585\n",
      "25/05/06 17:28:12 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[8] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/05/06 17:28:12 INFO TaskSchedulerImpl: Adding task set 3.0 with 1 tasks resource profile 0\n",
      "25/05/06 17:28:12 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 6) (172.21.73.132, executor driver, partition 0, NODE_LOCAL, 8999 bytes) \n",
      "25/05/06 17:28:12 INFO Executor: Running task 0.0 in stage 3.0 (TID 6)\n",
      "25/05/06 17:28:12 INFO ShuffleBlockFetcherIterator: Getting 5 (300.0 B) non-empty blocks including 5 (300.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "25/05/06 17:28:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 10 ms\n",
      "25/05/06 17:28:12 INFO CodeGenerator: Code generated in 11.519796 ms\n",
      "25/05/06 17:28:12 INFO Executor: Finished task 0.0 in stage 3.0 (TID 6). 4038 bytes result sent to driver\n",
      "25/05/06 17:28:12 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 6) in 83 ms on 172.21.73.132 (executor driver) (1/1)\n",
      "25/05/06 17:28:12 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool \n",
      "25/05/06 17:28:12 INFO DAGScheduler: ResultStage 3 (count at NativeMethodAccessorImpl.java:0) finished in 0.100 s\n",
      "25/05/06 17:28:12 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/05/06 17:28:12 INFO TaskSchedulerImpl: Killing all running tasks in stage 3: Stage finished\n",
      "25/05/06 17:28:12 INFO DAGScheduler: Job 2 finished: count at NativeMethodAccessorImpl.java:0, took 0.110693 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "55939"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = spark.read.parquet('cleaned_df_after_2018.parquet')\n",
    "data.count()\n",
    "data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9447c4b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- region: string (nullable = true)\n",
      " |-- issue_d: date (nullable = true)\n",
      " |-- loan_describe_int: integer (nullable = true)\n",
      " |-- last_fico_range_high: integer (nullable = true)\n",
      " |-- last_fico_range_low: integer (nullable = true)\n",
      " |-- int_rate: float (nullable = true)\n",
      " |-- fico_range_low: float (nullable = true)\n",
      " |-- acc_open_past_24mths: integer (nullable = true)\n",
      " |-- dti: float (nullable = true)\n",
      " |-- num_tl_op_past_12m: integer (nullable = true)\n",
      " |-- loan_amnt: float (nullable = true)\n",
      " |-- mort_acc: integer (nullable = true)\n",
      " |-- avg_cur_bal: float (nullable = true)\n",
      " |-- bc_open_to_buy: float (nullable = true)\n",
      " |-- num_actv_rev_tl: integer (nullable = true)\n",
      " |-- debt_settlement_flag: string (nullable = true)\n",
      " |-- term: string (nullable = true)\n",
      " |-- purpose: string (nullable = true)\n",
      " |-- hardship_flag: string (nullable = true)\n",
      " |-- pymnt_plan: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5f10e95b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Таблица 'loan_data' успешно создана в PostgreSQL\n"
     ]
    }
   ],
   "source": [
    "from sqlalchemy import create_engine, MetaData, Table, Column, String, Date, Integer, Float\n",
    "from sqlalchemy.dialects.postgresql import VARCHAR, DATE, INTEGER, FLOAT\n",
    "\n",
    "# Настройки подключения к PostgreSQL\n",
    "DB_USER = 'user'  # замените на ваше имя пользователя\n",
    "DB_PASSWORD = 'user'  # замените на ваш пароль\n",
    "DB_HOST = 'localhost'  # или '127.0.0.1'\n",
    "DB_PORT = '5431'  # стандартный порт PostgreSQL\n",
    "DB_NAME = 'user'  # замените на имя вашей базы данных\n",
    "\n",
    "# Строка подключения для PostgreSQL\n",
    "DATABASE_URL = f\"postgresql://{DB_USER}:{DB_PASSWORD}@{DB_HOST}:{DB_PORT}/{DB_NAME}\"\n",
    "\n",
    "# Создаем движок SQLAlchemy\n",
    "engine = create_engine(DATABASE_URL)\n",
    "\n",
    "# Создаем объект метаданных\n",
    "metadata = MetaData()\n",
    "\n",
    "# Определяем структуру таблицы\n",
    "loan_data_table = Table(\n",
    "    'loan_data', \n",
    "    metadata,\n",
    "    Column('id', Integer, primary_key=True, autoincrement=True),\n",
    "    Column('region', String(255), nullable=True),\n",
    "    Column('issue_d', Date, nullable=True),\n",
    "    Column('loan_describe_int', Integer, nullable=True),\n",
    "    Column('last_fico_range_high', Integer, nullable=True),\n",
    "    Column('last_fico_range_low', Integer, nullable=True),\n",
    "    Column('int_rate', Float, nullable=True),\n",
    "    Column('fico_range_low', Float, nullable=True),\n",
    "    Column('acc_open_past_24mths', Integer, nullable=True),\n",
    "    Column('dti', Float, nullable=True),\n",
    "    Column('num_tl_op_past_12m', Integer, nullable=True),\n",
    "    Column('loan_amnt', Float, nullable=True),\n",
    "    Column('mort_acc', Integer, nullable=True),\n",
    "    Column('avg_cur_bal', Float, nullable=True),\n",
    "    Column('bc_open_to_buy', Float, nullable=True),\n",
    "    Column('num_actv_rev_tl', Integer, nullable=True),\n",
    "    Column('debt_settlement_flag', String(255), nullable=True),\n",
    "    Column('term', String(255), nullable=True),\n",
    "    Column('purpose', String(255), nullable=True),\n",
    "    Column('hardship_flag', String(255), nullable=True),\n",
    "    Column('pymnt_plan', String(255), nullable=True)\n",
    ")\n",
    "\n",
    "# Создаем таблицу в базе данных\n",
    "metadata.create_all(engine)\n",
    "\n",
    "print(\"Таблица 'loan_data' успешно создана в PostgreSQL\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ff113119",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Преобразуем в Pandas DataFrame\n",
    "pdf = data.toPandas()\n",
    "\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "# Подключение к вашей PostgreSQL (из docker-compose)\n",
    "engine = create_engine(\"postgresql+psycopg2://user:user@localhost:5431/user\")\n",
    "\n",
    "# Вариант 2: Использование SQLAlchemy Core для большей гибкости\n",
    "from sqlalchemy import insert\n",
    "\n",
    "with engine.connect() as connection:\n",
    "    # Преобразуем DataFrame в список словарей\n",
    "    records = pdf.to_dict('records')\n",
    "    \n",
    "    # Выполняем вставку (id будет генерироваться автоматически)\n",
    "    connection.execute(\n",
    "        insert(loan_data_table),\n",
    "        records\n",
    "    )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "29daaa72",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import select\n",
    "\n",
    "with engine.connect() as connection:\n",
    "    # Создаем запрос с лимитом 100 записей\n",
    "    query = select(loan_data_table).limit(100)\n",
    "    \n",
    "    # Выполняем запрос\n",
    "    result = connection.execute(query)\n",
    "    \n",
    "    # Преобразуем результат в список словарей\n",
    "    first_100_records = [dict(row._mapping) for row in result]\n",
    "    \n",
    "    # Или в DataFrame (если нужно)\n",
    "    import pandas as pd\n",
    "    df = pd.DataFrame(first_100_records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8d08a0ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'region', 'issue_d', 'loan_describe_int', 'last_fico_range_high',\n",
       "       'last_fico_range_low', 'int_rate', 'fico_range_low',\n",
       "       'acc_open_past_24mths', 'dti', 'num_tl_op_past_12m', 'loan_amnt',\n",
       "       'mort_acc', 'avg_cur_bal', 'bc_open_to_buy', 'num_actv_rev_tl',\n",
       "       'debt_settlement_flag', 'term', 'purpose', 'hardship_flag',\n",
       "       'pymnt_plan'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "afc03bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_df = spark.createDataFrame(df)\n",
    "# spark_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fece11ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/06 17:29:25 INFO DAGScheduler: Registering RDD 59 (collect at StringIndexer.scala:204) as input to shuffle 7\n",
      "25/05/06 17:29:25 INFO DAGScheduler: Got map stage job 16 (collect at StringIndexer.scala:204) with 16 output partitions\n",
      "25/05/06 17:29:25 INFO DAGScheduler: Final stage: ShuffleMapStage 23 (collect at StringIndexer.scala:204)\n",
      "25/05/06 17:29:25 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/05/06 17:29:25 INFO DAGScheduler: Missing parents: List()\n",
      "25/05/06 17:29:25 INFO DAGScheduler: Submitting ShuffleMapStage 23 (MapPartitionsRDD[59] at collect at StringIndexer.scala:204), which has no missing parents\n",
      "25/05/06 17:29:25 INFO MemoryStore: Block broadcast_17 stored as values in memory (estimated size 31.9 KiB, free 2.2 GiB)\n",
      "25/05/06 17:29:25 INFO MemoryStore: Block broadcast_17_piece0 stored as bytes in memory (estimated size 15.1 KiB, free 2.2 GiB)\n",
      "25/05/06 17:29:25 INFO BlockManagerInfo: Added broadcast_17_piece0 in memory on 172.21.73.132:40669 (size: 15.1 KiB, free: 2.2 GiB)\n",
      "25/05/06 17:29:25 INFO SparkContext: Created broadcast 17 from broadcast at DAGScheduler.scala:1585\n",
      "25/05/06 17:29:25 INFO DAGScheduler: Submitting 16 missing tasks from ShuffleMapStage 23 (MapPartitionsRDD[59] at collect at StringIndexer.scala:204) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14))\n",
      "25/05/06 17:29:25 INFO TaskSchedulerImpl: Adding task set 23.0 with 16 tasks resource profile 0\n",
      "25/05/06 17:29:25 INFO TaskSetManager: Starting task 0.0 in stage 23.0 (TID 110) (172.21.73.132, executor driver, partition 0, PROCESS_LOCAL, 9753 bytes) \n",
      "25/05/06 17:29:25 INFO TaskSetManager: Starting task 1.0 in stage 23.0 (TID 111) (172.21.73.132, executor driver, partition 1, PROCESS_LOCAL, 9754 bytes) \n",
      "25/05/06 17:29:25 INFO TaskSetManager: Starting task 2.0 in stage 23.0 (TID 112) (172.21.73.132, executor driver, partition 2, PROCESS_LOCAL, 9761 bytes) \n",
      "25/05/06 17:29:25 INFO TaskSetManager: Starting task 3.0 in stage 23.0 (TID 113) (172.21.73.132, executor driver, partition 3, PROCESS_LOCAL, 9760 bytes) \n",
      "25/05/06 17:29:25 INFO TaskSetManager: Starting task 4.0 in stage 23.0 (TID 114) (172.21.73.132, executor driver, partition 4, PROCESS_LOCAL, 9779 bytes) \n",
      "25/05/06 17:29:25 INFO TaskSetManager: Starting task 5.0 in stage 23.0 (TID 115) (172.21.73.132, executor driver, partition 5, PROCESS_LOCAL, 9771 bytes) \n",
      "25/05/06 17:29:25 INFO TaskSetManager: Starting task 6.0 in stage 23.0 (TID 116) (172.21.73.132, executor driver, partition 6, PROCESS_LOCAL, 9748 bytes) \n",
      "25/05/06 17:29:25 INFO TaskSetManager: Starting task 7.0 in stage 23.0 (TID 117) (172.21.73.132, executor driver, partition 7, PROCESS_LOCAL, 9757 bytes) \n",
      "25/05/06 17:29:25 INFO TaskSetManager: Starting task 8.0 in stage 23.0 (TID 118) (172.21.73.132, executor driver, partition 8, PROCESS_LOCAL, 9775 bytes) \n",
      "25/05/06 17:29:25 INFO TaskSetManager: Starting task 9.0 in stage 23.0 (TID 119) (172.21.73.132, executor driver, partition 9, PROCESS_LOCAL, 9751 bytes) \n",
      "25/05/06 17:29:25 INFO TaskSetManager: Starting task 10.0 in stage 23.0 (TID 120) (172.21.73.132, executor driver, partition 10, PROCESS_LOCAL, 9764 bytes) \n",
      "25/05/06 17:29:25 INFO TaskSetManager: Starting task 11.0 in stage 23.0 (TID 121) (172.21.73.132, executor driver, partition 11, PROCESS_LOCAL, 9771 bytes) \n",
      "25/05/06 17:29:25 INFO TaskSetManager: Starting task 12.0 in stage 23.0 (TID 122) (172.21.73.132, executor driver, partition 12, PROCESS_LOCAL, 9771 bytes) \n",
      "25/05/06 17:29:25 INFO TaskSetManager: Starting task 13.0 in stage 23.0 (TID 123) (172.21.73.132, executor driver, partition 13, PROCESS_LOCAL, 9768 bytes) \n",
      "25/05/06 17:29:25 INFO TaskSetManager: Starting task 14.0 in stage 23.0 (TID 124) (172.21.73.132, executor driver, partition 14, PROCESS_LOCAL, 9766 bytes) \n",
      "25/05/06 17:29:25 INFO TaskSetManager: Starting task 15.0 in stage 23.0 (TID 125) (172.21.73.132, executor driver, partition 15, PROCESS_LOCAL, 10297 bytes) \n",
      "25/05/06 17:29:25 INFO Executor: Running task 1.0 in stage 23.0 (TID 111)\n",
      "25/05/06 17:29:25 INFO Executor: Running task 4.0 in stage 23.0 (TID 114)\n",
      "25/05/06 17:29:25 INFO Executor: Running task 0.0 in stage 23.0 (TID 110)\n",
      "25/05/06 17:29:25 INFO Executor: Running task 5.0 in stage 23.0 (TID 115)\n",
      "25/05/06 17:29:25 INFO Executor: Running task 8.0 in stage 23.0 (TID 118)\n",
      "25/05/06 17:29:25 INFO Executor: Running task 2.0 in stage 23.0 (TID 112)\n",
      "25/05/06 17:29:25 INFO Executor: Running task 15.0 in stage 23.0 (TID 125)\n",
      "25/05/06 17:29:25 INFO Executor: Running task 3.0 in stage 23.0 (TID 113)\n",
      "25/05/06 17:29:25 INFO Executor: Running task 9.0 in stage 23.0 (TID 119)\n",
      "25/05/06 17:29:25 INFO Executor: Running task 10.0 in stage 23.0 (TID 120)\n",
      "25/05/06 17:29:25 INFO Executor: Running task 11.0 in stage 23.0 (TID 121)\n",
      "25/05/06 17:29:25 INFO Executor: Running task 14.0 in stage 23.0 (TID 124)\n",
      "25/05/06 17:29:25 INFO Executor: Running task 6.0 in stage 23.0 (TID 116)\n",
      "25/05/06 17:29:25 INFO Executor: Running task 13.0 in stage 23.0 (TID 123)\n",
      "25/05/06 17:29:25 INFO Executor: Running task 7.0 in stage 23.0 (TID 117)\n",
      "25/05/06 17:29:25 INFO Executor: Running task 12.0 in stage 23.0 (TID 122)\n",
      "25/05/06 17:29:25 INFO PythonRunner: Times: total = 56, boot = -30867, init = 30923, finish = 0\n",
      "25/05/06 17:29:25 INFO PythonRunner: Times: total = 55, boot = -30839, init = 30893, finish = 1\n",
      "25/05/06 17:29:25 INFO PythonRunner: Times: total = 57, boot = -30888, init = 30945, finish = 0\n",
      "25/05/06 17:29:25 INFO PythonRunner: Times: total = 54, boot = -30899, init = 30953, finish = 0\n",
      "25/05/06 17:29:25 INFO Executor: Finished task 9.0 in stage 23.0 (TID 119). 2436 bytes result sent to driver\n",
      "25/05/06 17:29:25 INFO TaskSetManager: Finished task 9.0 in stage 23.0 (TID 119) in 73 ms on 172.21.73.132 (executor driver) (1/16)\n",
      "25/05/06 17:29:25 INFO PythonRunner: Times: total = 63, boot = -30835, init = 30898, finish = 0\n",
      "25/05/06 17:29:25 INFO Executor: Finished task 1.0 in stage 23.0 (TID 111). 2436 bytes result sent to driver\n",
      "25/05/06 17:29:25 INFO TaskSetManager: Finished task 1.0 in stage 23.0 (TID 111) in 81 ms on 172.21.73.132 (executor driver) (2/16)\n",
      "25/05/06 17:29:25 INFO PythonRunner: Times: total = 70, boot = -30866, init = 30936, finish = 0\n",
      "25/05/06 17:29:25 INFO Executor: Finished task 8.0 in stage 23.0 (TID 118). 2436 bytes result sent to driver\n",
      "25/05/06 17:29:25 INFO Executor: Finished task 11.0 in stage 23.0 (TID 121). 2436 bytes result sent to driver\n",
      "25/05/06 17:29:25 INFO TaskSetManager: Finished task 11.0 in stage 23.0 (TID 121) in 91 ms on 172.21.73.132 (executor driver) (3/16)\n",
      "25/05/06 17:29:25 INFO PythonRunner: Times: total = 78, boot = -30897, init = 30975, finish = 0\n",
      "25/05/06 17:29:25 INFO PythonRunner: Times: total = 81, boot = -30856, init = 30936, finish = 1\n",
      "25/05/06 17:29:25 INFO TaskSetManager: Finished task 8.0 in stage 23.0 (TID 118) in 92 ms on 172.21.73.132 (executor driver) (4/16)\n",
      "25/05/06 17:29:25 INFO Executor: Finished task 4.0 in stage 23.0 (TID 114). 2436 bytes result sent to driver\n",
      "25/05/06 17:29:25 INFO TaskSetManager: Finished task 4.0 in stage 23.0 (TID 114) in 93 ms on 172.21.73.132 (executor driver) (5/16)\n",
      "25/05/06 17:29:25 INFO Executor: Finished task 10.0 in stage 23.0 (TID 120). 2436 bytes result sent to driver\n",
      "25/05/06 17:29:25 INFO TaskSetManager: Finished task 10.0 in stage 23.0 (TID 120) in 95 ms on 172.21.73.132 (executor driver) (6/16)\n",
      "25/05/06 17:29:25 INFO PythonRunner: Times: total = 59, boot = -30839, init = 30898, finish = 0\n",
      "25/05/06 17:29:25 INFO PythonRunner: Times: total = 87, boot = -30861, init = 30948, finish = 0\n",
      "25/05/06 17:29:25 INFO PythonRunner: Times: total = 68, boot = -30879, init = 30947, finish = 0\n",
      "25/05/06 17:29:25 INFO Executor: Finished task 13.0 in stage 23.0 (TID 123). 2436 bytes result sent to driver\n",
      "25/05/06 17:29:25 INFO TaskSetManager: Finished task 13.0 in stage 23.0 (TID 123) in 103 ms on 172.21.73.132 (executor driver) (7/16)\n",
      "25/05/06 17:29:25 INFO Executor: Finished task 5.0 in stage 23.0 (TID 115). 2436 bytes result sent to driver\n",
      "25/05/06 17:29:25 INFO Executor: Finished task 3.0 in stage 23.0 (TID 113). 2436 bytes result sent to driver\n",
      "25/05/06 17:29:25 INFO PythonRunner: Times: total = 88, boot = -30806, init = 30894, finish = 0\n",
      "25/05/06 17:29:25 INFO TaskSetManager: Finished task 5.0 in stage 23.0 (TID 115) in 109 ms on 172.21.73.132 (executor driver) (8/16)\n",
      "25/05/06 17:29:25 INFO TaskSetManager: Finished task 3.0 in stage 23.0 (TID 113) in 109 ms on 172.21.73.132 (executor driver) (9/16)\n",
      "25/05/06 17:29:25 INFO Executor: Finished task 2.0 in stage 23.0 (TID 112). 2436 bytes result sent to driver\n",
      "25/05/06 17:29:25 INFO Executor: Finished task 12.0 in stage 23.0 (TID 122). 2436 bytes result sent to driver\n",
      "25/05/06 17:29:25 INFO TaskSetManager: Finished task 2.0 in stage 23.0 (TID 112) in 112 ms on 172.21.73.132 (executor driver) (10/16)\n",
      "25/05/06 17:29:25 INFO TaskSetManager: Finished task 12.0 in stage 23.0 (TID 122) in 112 ms on 172.21.73.132 (executor driver) (11/16)\n",
      "25/05/06 17:29:25 INFO Executor: Finished task 15.0 in stage 23.0 (TID 125). 2436 bytes result sent to driver\n",
      "25/05/06 17:29:25 INFO TaskSetManager: Finished task 15.0 in stage 23.0 (TID 125) in 117 ms on 172.21.73.132 (executor driver) (12/16)\n",
      "25/05/06 17:29:25 INFO PythonRunner: Times: total = 108, boot = -30838, init = 30946, finish = 0\n",
      "25/05/06 17:29:25 INFO PythonRunner: Times: total = 100, boot = -30872, init = 30972, finish = 0\n",
      "25/05/06 17:29:25 INFO Executor: Finished task 0.0 in stage 23.0 (TID 110). 2436 bytes result sent to driver\n",
      "25/05/06 17:29:25 INFO TaskSetManager: Finished task 0.0 in stage 23.0 (TID 110) in 131 ms on 172.21.73.132 (executor driver) (13/16)\n",
      "25/05/06 17:29:25 INFO Executor: Finished task 7.0 in stage 23.0 (TID 117). 2436 bytes result sent to driver\n",
      "25/05/06 17:29:25 INFO TaskSetManager: Finished task 7.0 in stage 23.0 (TID 117) in 132 ms on 172.21.73.132 (executor driver) (14/16)\n",
      "25/05/06 17:29:25 INFO PythonRunner: Times: total = 108, boot = 2, init = 106, finish = 0\n",
      "25/05/06 17:29:25 INFO PythonRunner: Times: total = 125, boot = -30813, init = 30938, finish = 0\n",
      "25/05/06 17:29:25 INFO Executor: Finished task 14.0 in stage 23.0 (TID 124). 2436 bytes result sent to driver\n",
      "25/05/06 17:29:25 INFO TaskSetManager: Finished task 14.0 in stage 23.0 (TID 124) in 139 ms on 172.21.73.132 (executor driver) (15/16)\n",
      "25/05/06 17:29:25 INFO Executor: Finished task 6.0 in stage 23.0 (TID 116). 2436 bytes result sent to driver\n",
      "25/05/06 17:29:25 INFO TaskSetManager: Finished task 6.0 in stage 23.0 (TID 116) in 146 ms on 172.21.73.132 (executor driver) (16/16)\n",
      "25/05/06 17:29:25 INFO TaskSchedulerImpl: Removed TaskSet 23.0, whose tasks have all completed, from pool \n",
      "25/05/06 17:29:25 INFO DAGScheduler: ShuffleMapStage 23 (collect at StringIndexer.scala:204) finished in 0.156 s\n",
      "25/05/06 17:29:25 INFO DAGScheduler: looking for newly runnable stages\n",
      "25/05/06 17:29:25 INFO DAGScheduler: running: Set()\n",
      "25/05/06 17:29:25 INFO DAGScheduler: waiting: Set()\n",
      "25/05/06 17:29:25 INFO DAGScheduler: failed: Set()\n",
      "25/05/06 17:29:25 INFO BlockManagerInfo: Removed broadcast_16_piece0 on 172.21.73.132:40669 in memory (size: 28.4 KiB, free: 2.2 GiB)\n",
      "25/05/06 17:29:25 INFO BlockManagerInfo: Removed broadcast_17_piece0 on 172.21.73.132:40669 in memory (size: 15.1 KiB, free: 2.2 GiB)\n",
      "25/05/06 17:29:25 INFO SparkContext: Starting job: collect at StringIndexer.scala:204\n",
      "25/05/06 17:29:25 INFO DAGScheduler: Got job 17 (collect at StringIndexer.scala:204) with 1 output partitions\n",
      "25/05/06 17:29:25 INFO DAGScheduler: Final stage: ResultStage 25 (collect at StringIndexer.scala:204)\n",
      "25/05/06 17:29:25 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 24)\n",
      "25/05/06 17:29:25 INFO DAGScheduler: Missing parents: List()\n",
      "25/05/06 17:29:25 INFO DAGScheduler: Submitting ResultStage 25 (MapPartitionsRDD[62] at collect at StringIndexer.scala:204), which has no missing parents\n",
      "25/05/06 17:29:25 INFO MemoryStore: Block broadcast_18 stored as values in memory (estimated size 35.6 KiB, free 2.2 GiB)\n",
      "25/05/06 17:29:25 INFO MemoryStore: Block broadcast_18_piece0 stored as bytes in memory (estimated size 17.1 KiB, free 2.2 GiB)\n",
      "25/05/06 17:29:25 INFO BlockManagerInfo: Added broadcast_18_piece0 in memory on 172.21.73.132:40669 (size: 17.1 KiB, free: 2.2 GiB)\n",
      "25/05/06 17:29:25 INFO SparkContext: Created broadcast 18 from broadcast at DAGScheduler.scala:1585\n",
      "25/05/06 17:29:25 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 25 (MapPartitionsRDD[62] at collect at StringIndexer.scala:204) (first 15 tasks are for partitions Vector(0))\n",
      "25/05/06 17:29:25 INFO TaskSchedulerImpl: Adding task set 25.0 with 1 tasks resource profile 0\n",
      "25/05/06 17:29:25 INFO TaskSetManager: Starting task 0.0 in stage 25.0 (TID 126) (172.21.73.132, executor driver, partition 0, NODE_LOCAL, 8999 bytes) \n",
      "25/05/06 17:29:25 INFO Executor: Running task 0.0 in stage 25.0 (TID 126)\n",
      "25/05/06 17:29:25 INFO ShuffleBlockFetcherIterator: Getting 16 (5.2 KiB) non-empty blocks including 16 (5.2 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "25/05/06 17:29:25 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\n",
      "25/05/06 17:29:25 INFO Executor: Finished task 0.0 in stage 25.0 (TID 126). 5108 bytes result sent to driver\n",
      "25/05/06 17:29:25 INFO TaskSetManager: Finished task 0.0 in stage 25.0 (TID 126) in 16 ms on 172.21.73.132 (executor driver) (1/1)\n",
      "25/05/06 17:29:25 INFO TaskSchedulerImpl: Removed TaskSet 25.0, whose tasks have all completed, from pool \n",
      "25/05/06 17:29:25 INFO DAGScheduler: ResultStage 25 (collect at StringIndexer.scala:204) finished in 0.020 s\n",
      "25/05/06 17:29:25 INFO DAGScheduler: Job 17 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/05/06 17:29:25 INFO TaskSchedulerImpl: Killing all running tasks in stage 25: Stage finished\n",
      "25/05/06 17:29:25 INFO DAGScheduler: Job 17 finished: collect at StringIndexer.scala:204, took 0.022439 s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------------------------------------------------------------------------+-----------------+\n",
      "|features                                                                                                 |loan_describe_int|\n",
      "+---------------------------------------------------------------------------------------------------------+-----------------+\n",
      "|[679.0,675.0,11.979999542236328,675.0,2.0,14.180000305175781,2.0,5000.0,0.0,2222.0,10551.0,3.0,1.0,0.0]  |1                |\n",
      "|[699.0,695.0,21.850000381469727,680.0,5.0,27.579999923706055,5.0,30000.0,2.0,14324.0,19752.0,4.0,1.0,0.0]|1                |\n",
      "|[729.0,725.0,14.069999694824219,720.0,1.0,24.690000534057617,0.0,10000.0,0.0,5524.0,3793.0,1.0,0.0,1.0]  |1                |\n",
      "|[799.0,795.0,7.340000152587891,725.0,5.0,18.649999618530273,5.0,23000.0,2.0,8466.0,22215.0,8.0,1.0,0.0]  |1                |\n",
      "|[674.0,670.0,20.389999389648438,720.0,10.0,26.530000686645508,7.0,14150.0,4.0,5843.0,30261.0,5.0,0.0,1.0]|0                |\n",
      "+---------------------------------------------------------------------------------------------------------+-----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/06 17:29:25 INFO CodeGenerator: Code generated in 9.962181 ms\n",
      "25/05/06 17:29:25 INFO SparkContext: Starting job: showString at NativeMethodAccessorImpl.java:0\n",
      "25/05/06 17:29:25 INFO DAGScheduler: Got job 18 (showString at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/05/06 17:29:25 INFO DAGScheduler: Final stage: ResultStage 26 (showString at NativeMethodAccessorImpl.java:0)\n",
      "25/05/06 17:29:25 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/05/06 17:29:25 INFO DAGScheduler: Missing parents: List()\n",
      "25/05/06 17:29:25 INFO DAGScheduler: Submitting ResultStage 26 (MapPartitionsRDD[64] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/05/06 17:29:25 INFO MemoryStore: Block broadcast_19 stored as values in memory (estimated size 52.3 KiB, free 2.2 GiB)\n",
      "25/05/06 17:29:25 INFO MemoryStore: Block broadcast_19_piece0 stored as bytes in memory (estimated size 22.1 KiB, free 2.2 GiB)\n",
      "25/05/06 17:29:25 INFO BlockManagerInfo: Added broadcast_19_piece0 in memory on 172.21.73.132:40669 (size: 22.1 KiB, free: 2.2 GiB)\n",
      "25/05/06 17:29:25 INFO SparkContext: Created broadcast 19 from broadcast at DAGScheduler.scala:1585\n",
      "25/05/06 17:29:25 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 26 (MapPartitionsRDD[64] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/05/06 17:29:25 INFO TaskSchedulerImpl: Adding task set 26.0 with 1 tasks resource profile 0\n",
      "25/05/06 17:29:25 INFO TaskSetManager: Starting task 0.0 in stage 26.0 (TID 127) (172.21.73.132, executor driver, partition 0, PROCESS_LOCAL, 9764 bytes) \n",
      "25/05/06 17:29:25 INFO Executor: Running task 0.0 in stage 26.0 (TID 127)\n",
      "25/05/06 17:29:25 INFO CodeGenerator: Code generated in 18.251217 ms\n",
      "25/05/06 17:29:25 INFO Executor: Finished task 0.0 in stage 26.0 (TID 127). 2341 bytes result sent to driver\n",
      "25/05/06 17:29:25 INFO TaskSetManager: Finished task 0.0 in stage 26.0 (TID 127) in 67 ms on 172.21.73.132 (executor driver) (1/1)\n",
      "25/05/06 17:29:25 INFO TaskSchedulerImpl: Removed TaskSet 26.0, whose tasks have all completed, from pool \n",
      "25/05/06 17:29:25 INFO DAGScheduler: ResultStage 26 (showString at NativeMethodAccessorImpl.java:0) finished in 0.072 s\n",
      "25/05/06 17:29:25 INFO DAGScheduler: Job 18 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/05/06 17:29:25 INFO TaskSchedulerImpl: Killing all running tasks in stage 26: Stage finished\n",
      "25/05/06 17:29:25 INFO DAGScheduler: Job 18 finished: showString at NativeMethodAccessorImpl.java:0, took 0.074622 s\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "\n",
    "\n",
    "all_columns = spark_df.columns\n",
    "target_column = \"loan_describe_int\"\n",
    "feature_columns = [col for col in all_columns if col != target_column]\n",
    "\n",
    "categorical_columns = [\"term\"]\n",
    "\n",
    "indexers = [StringIndexer(inputCol=col, outputCol=f\"{col}_index\", handleInvalid=\"keep\") for col in categorical_columns]\n",
    "\n",
    "encoders = [OneHotEncoder(inputCol=f\"{col}_index\", outputCol=f\"{col}_encoded\") for col in categorical_columns]\n",
    "\n",
    "pipeline = Pipeline(stages=indexers + encoders)\n",
    "\n",
    "df_encoded = pipeline.fit(spark_df).transform(spark_df)\n",
    "\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "numeric_columns = [\n",
    "    \"last_fico_range_high\", \"last_fico_range_low\", \"int_rate\", \"fico_range_low\",\n",
    "    \"acc_open_past_24mths\", \"dti\", \"num_tl_op_past_12m\", \"loan_amnt\",\n",
    "    \"mort_acc\", \"avg_cur_bal\", \"bc_open_to_buy\", \"num_actv_rev_tl\"\n",
    "]\n",
    "\n",
    "encoded_columns = [f\"{col}_encoded\" for col in categorical_columns]\n",
    "final_feature_columns = numeric_columns + encoded_columns\n",
    "assembler = VectorAssembler(inputCols=final_feature_columns, outputCol=\"features\")\n",
    "final_data = assembler.transform(df_encoded).select(\"features\", target_column)\n",
    "\n",
    "\n",
    "final_data.show(truncate=False, n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "85b3b0c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/06 17:29:36 INFO MemoryStore: Block broadcast_20 stored as values in memory (estimated size 221.7 KiB, free 2.2 GiB)\n",
      "25/05/06 17:29:36 INFO MemoryStore: Block broadcast_20_piece0 stored as bytes in memory (estimated size 32.7 KiB, free 2.2 GiB)\n",
      "25/05/06 17:29:36 INFO BlockManagerInfo: Added broadcast_20_piece0 in memory on 172.21.73.132:40669 (size: 32.7 KiB, free: 2.2 GiB)\n",
      "25/05/06 17:29:36 INFO SparkContext: Created broadcast 20 from textFile at ReadWrite.scala:587\n",
      "25/05/06 17:29:36 INFO FileInputFormat: Total input files to process : 1\n",
      "25/05/06 17:29:36 INFO SparkContext: Starting job: first at ReadWrite.scala:587\n",
      "25/05/06 17:29:36 INFO DAGScheduler: Got job 19 (first at ReadWrite.scala:587) with 1 output partitions\n",
      "25/05/06 17:29:36 INFO DAGScheduler: Final stage: ResultStage 27 (first at ReadWrite.scala:587)\n",
      "25/05/06 17:29:36 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/05/06 17:29:36 INFO DAGScheduler: Missing parents: List()\n",
      "25/05/06 17:29:36 INFO DAGScheduler: Submitting ResultStage 27 (logistic_regression_model/metadata MapPartitionsRDD[66] at textFile at ReadWrite.scala:587), which has no missing parents\n",
      "25/05/06 17:29:36 INFO MemoryStore: Block broadcast_21 stored as values in memory (estimated size 4.9 KiB, free 2.2 GiB)\n",
      "25/05/06 17:29:36 INFO MemoryStore: Block broadcast_21_piece0 stored as bytes in memory (estimated size 2.8 KiB, free 2.2 GiB)\n",
      "25/05/06 17:29:36 INFO BlockManagerInfo: Added broadcast_21_piece0 in memory on 172.21.73.132:40669 (size: 2.8 KiB, free: 2.2 GiB)\n",
      "25/05/06 17:29:36 INFO SparkContext: Created broadcast 21 from broadcast at DAGScheduler.scala:1585\n",
      "25/05/06 17:29:36 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 27 (logistic_regression_model/metadata MapPartitionsRDD[66] at textFile at ReadWrite.scala:587) (first 15 tasks are for partitions Vector(0))\n",
      "25/05/06 17:29:36 INFO TaskSchedulerImpl: Adding task set 27.0 with 1 tasks resource profile 0\n",
      "25/05/06 17:29:36 INFO TaskSetManager: Starting task 0.0 in stage 27.0 (TID 128) (172.21.73.132, executor driver, partition 0, PROCESS_LOCAL, 9087 bytes) \n",
      "25/05/06 17:29:36 INFO Executor: Running task 0.0 in stage 27.0 (TID 128)\n",
      "25/05/06 17:29:36 INFO HadoopRDD: Input split: file:/home/diploma/Diploma/logistic_regression_model/metadata/part-00000:0+571\n",
      "25/05/06 17:29:36 INFO Executor: Finished task 0.0 in stage 27.0 (TID 128). 1511 bytes result sent to driver\n",
      "25/05/06 17:29:36 INFO TaskSetManager: Finished task 0.0 in stage 27.0 (TID 128) in 26 ms on 172.21.73.132 (executor driver) (1/1)\n",
      "25/05/06 17:29:36 INFO TaskSchedulerImpl: Removed TaskSet 27.0, whose tasks have all completed, from pool \n",
      "25/05/06 17:29:36 INFO DAGScheduler: ResultStage 27 (first at ReadWrite.scala:587) finished in 0.032 s\n",
      "25/05/06 17:29:36 INFO DAGScheduler: Job 19 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/05/06 17:29:36 INFO TaskSchedulerImpl: Killing all running tasks in stage 27: Stage finished\n",
      "25/05/06 17:29:36 INFO DAGScheduler: Job 19 finished: first at ReadWrite.scala:587, took 0.035102 s\n",
      "25/05/06 17:29:36 INFO InMemoryFileIndex: It took 1 ms to list leaf files for 1 paths.\n",
      "25/05/06 17:29:36 INFO SparkContext: Starting job: load at LogisticRegression.scala:1330\n",
      "25/05/06 17:29:36 INFO DAGScheduler: Got job 20 (load at LogisticRegression.scala:1330) with 1 output partitions\n",
      "25/05/06 17:29:36 INFO DAGScheduler: Final stage: ResultStage 28 (load at LogisticRegression.scala:1330)\n",
      "25/05/06 17:29:36 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/05/06 17:29:36 INFO DAGScheduler: Missing parents: List()\n",
      "25/05/06 17:29:36 INFO DAGScheduler: Submitting ResultStage 28 (MapPartitionsRDD[68] at load at LogisticRegression.scala:1330), which has no missing parents\n",
      "25/05/06 17:29:36 INFO MemoryStore: Block broadcast_22 stored as values in memory (estimated size 103.7 KiB, free 2.2 GiB)\n",
      "25/05/06 17:29:36 INFO MemoryStore: Block broadcast_22_piece0 stored as bytes in memory (estimated size 37.4 KiB, free 2.2 GiB)\n",
      "25/05/06 17:29:36 INFO BlockManagerInfo: Added broadcast_22_piece0 in memory on 172.21.73.132:40669 (size: 37.4 KiB, free: 2.2 GiB)\n",
      "25/05/06 17:29:36 INFO SparkContext: Created broadcast 22 from broadcast at DAGScheduler.scala:1585\n",
      "25/05/06 17:29:36 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 28 (MapPartitionsRDD[68] at load at LogisticRegression.scala:1330) (first 15 tasks are for partitions Vector(0))\n",
      "25/05/06 17:29:36 INFO TaskSchedulerImpl: Adding task set 28.0 with 1 tasks resource profile 0\n",
      "25/05/06 17:29:36 INFO TaskSetManager: Starting task 0.0 in stage 28.0 (TID 129) (172.21.73.132, executor driver, partition 0, PROCESS_LOCAL, 9219 bytes) \n",
      "25/05/06 17:29:36 INFO Executor: Running task 0.0 in stage 28.0 (TID 129)\n",
      "25/05/06 17:29:36 INFO Executor: Finished task 0.0 in stage 28.0 (TID 129). 2401 bytes result sent to driver\n",
      "25/05/06 17:29:36 INFO TaskSetManager: Finished task 0.0 in stage 28.0 (TID 129) in 24 ms on 172.21.73.132 (executor driver) (1/1)\n",
      "25/05/06 17:29:36 INFO TaskSchedulerImpl: Removed TaskSet 28.0, whose tasks have all completed, from pool \n",
      "25/05/06 17:29:36 INFO DAGScheduler: ResultStage 28 (load at LogisticRegression.scala:1330) finished in 0.032 s\n",
      "25/05/06 17:29:36 INFO DAGScheduler: Job 20 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/05/06 17:29:36 INFO TaskSchedulerImpl: Killing all running tasks in stage 28: Stage finished\n",
      "25/05/06 17:29:36 INFO DAGScheduler: Job 20 finished: load at LogisticRegression.scala:1330, took 0.036087 s\n",
      "25/05/06 17:29:36 INFO FileSourceStrategy: Pushed Filters: \n",
      "25/05/06 17:29:36 INFO FileSourceStrategy: Post-Scan Filters: \n",
      "25/05/06 17:29:36 INFO CodeGenerator: Code generated in 26.781915 ms\n",
      "25/05/06 17:29:36 INFO MemoryStore: Block broadcast_23 stored as values in memory (estimated size 204.7 KiB, free 2.2 GiB)\n",
      "25/05/06 17:29:36 INFO MemoryStore: Block broadcast_23_piece0 stored as bytes in memory (estimated size 36.0 KiB, free 2.2 GiB)\n",
      "25/05/06 17:29:36 INFO BlockManagerInfo: Added broadcast_23_piece0 in memory on 172.21.73.132:40669 (size: 36.0 KiB, free: 2.2 GiB)\n",
      "25/05/06 17:29:36 INFO SparkContext: Created broadcast 23 from head at LogisticRegression.scala:1348\n",
      "25/05/06 17:29:36 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "25/05/06 17:29:36 INFO SparkContext: Starting job: head at LogisticRegression.scala:1348\n",
      "25/05/06 17:29:36 INFO DAGScheduler: Got job 21 (head at LogisticRegression.scala:1348) with 1 output partitions\n",
      "25/05/06 17:29:36 INFO DAGScheduler: Final stage: ResultStage 29 (head at LogisticRegression.scala:1348)\n",
      "25/05/06 17:29:36 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/05/06 17:29:36 INFO DAGScheduler: Missing parents: List()\n",
      "25/05/06 17:29:36 INFO DAGScheduler: Submitting ResultStage 29 (MapPartitionsRDD[72] at head at LogisticRegression.scala:1348), which has no missing parents\n",
      "25/05/06 17:29:36 INFO MemoryStore: Block broadcast_24 stored as values in memory (estimated size 25.8 KiB, free 2.2 GiB)\n",
      "25/05/06 17:29:36 INFO MemoryStore: Block broadcast_24_piece0 stored as bytes in memory (estimated size 8.6 KiB, free 2.2 GiB)\n",
      "25/05/06 17:29:36 INFO BlockManagerInfo: Added broadcast_24_piece0 in memory on 172.21.73.132:40669 (size: 8.6 KiB, free: 2.2 GiB)\n",
      "25/05/06 17:29:36 INFO SparkContext: Created broadcast 24 from broadcast at DAGScheduler.scala:1585\n",
      "25/05/06 17:29:36 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 29 (MapPartitionsRDD[72] at head at LogisticRegression.scala:1348) (first 15 tasks are for partitions Vector(0))\n",
      "25/05/06 17:29:36 INFO TaskSchedulerImpl: Adding task set 29.0 with 1 tasks resource profile 0\n",
      "25/05/06 17:29:36 INFO TaskSetManager: Starting task 0.0 in stage 29.0 (TID 130) (172.21.73.132, executor driver, partition 0, PROCESS_LOCAL, 9689 bytes) \n",
      "25/05/06 17:29:36 INFO Executor: Running task 0.0 in stage 29.0 (TID 130)\n",
      "25/05/06 17:29:36 INFO CodeGenerator: Code generated in 19.171139 ms\n",
      "25/05/06 17:29:36 INFO FileScanRDD: Reading File path: file:///home/diploma/Diploma/logistic_regression_model/data/part-00000-939ca6d0-1d32-450b-9a51-c3e034a5c471-c000.snappy.parquet, range: 0-4659, partition values: [empty row]\n",
      "25/05/06 17:29:36 INFO CodecPool: Got brand-new decompressor [.snappy]\n",
      "25/05/06 17:29:36 INFO Executor: Finished task 0.0 in stage 29.0 (TID 130). 1911 bytes result sent to driver\n",
      "25/05/06 17:29:36 INFO TaskSetManager: Finished task 0.0 in stage 29.0 (TID 130) in 298 ms on 172.21.73.132 (executor driver) (1/1)\n",
      "25/05/06 17:29:36 INFO TaskSchedulerImpl: Removed TaskSet 29.0, whose tasks have all completed, from pool \n",
      "25/05/06 17:29:36 INFO DAGScheduler: ResultStage 29 (head at LogisticRegression.scala:1348) finished in 0.307 s\n",
      "25/05/06 17:29:36 INFO DAGScheduler: Job 21 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/05/06 17:29:36 INFO TaskSchedulerImpl: Killing all running tasks in stage 29: Stage finished\n",
      "25/05/06 17:29:36 INFO DAGScheduler: Job 21 finished: head at LogisticRegression.scala:1348, took 0.312256 s\n",
      "25/05/06 17:29:36 INFO CodeGenerator: Code generated in 9.979045 ms\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import LogisticRegressionModel\n",
    "\n",
    "model_save_path = \"logistic_regression_model\"\n",
    "loaded_model = LogisticRegressionModel.load(model_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d6e78f44",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_2018 = loaded_model.transform(final_data)\n",
    "\n",
    "# print(\"Sample predictions for 2018 data:\")\n",
    "# predictions_2018.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "11131f19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- features: vector (nullable = true)\n",
      " |-- loan_describe_int: long (nullable = true)\n",
      " |-- rawPrediction: vector (nullable = true)\n",
      " |-- probability: vector (nullable = true)\n",
      " |-- prediction: double (nullable = false)\n",
      "\n",
      "+---------------------------------------------------------------------------------------------------------+-----------------+----------------------------------------+-----------------------------------------+----------+\n",
      "|features                                                                                                 |loan_describe_int|rawPrediction                           |probability                              |prediction|\n",
      "+---------------------------------------------------------------------------------------------------------+-----------------+----------------------------------------+-----------------------------------------+----------+\n",
      "|[679.0,675.0,11.979999542236328,675.0,2.0,14.180000305175781,2.0,5000.0,0.0,2222.0,10551.0,3.0,1.0,0.0]  |1                |[-3.248226068800303,3.248226068800303]  |[0.037390683405916954,0.962609316594083] |1.0       |\n",
      "|[699.0,695.0,21.850000381469727,680.0,5.0,27.579999923706055,5.0,30000.0,2.0,14324.0,19752.0,4.0,1.0,0.0]|1                |[-2.448883460308142,2.448883460308142]  |[0.07952023792273866,0.9204797620772613] |1.0       |\n",
      "|[729.0,725.0,14.069999694824219,720.0,1.0,24.690000534057617,0.0,10000.0,0.0,5524.0,3793.0,1.0,0.0,1.0]  |1                |[-3.3478126599707956,3.3478126599707956]|[0.03396686448464579,0.9660331355153542] |1.0       |\n",
      "|[799.0,795.0,7.340000152587891,725.0,5.0,18.649999618530273,5.0,23000.0,2.0,8466.0,22215.0,8.0,1.0,0.0]  |1                |[-6.176302389492381,6.176302389492381]  |[0.002073788127413328,0.9979262118725867]|1.0       |\n",
      "|[674.0,670.0,20.389999389648438,720.0,10.0,26.530000686645508,7.0,14150.0,4.0,5843.0,30261.0,5.0,0.0,1.0]|0                |[-1.1013041734218767,1.1013041734218767]|[0.24949561135227327,0.7505043886477267] |1.0       |\n",
      "+---------------------------------------------------------------------------------------------------------+-----------------+----------------------------------------+-----------------------------------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/06 17:29:48 INFO CodeGenerator: Code generated in 14.685198 ms\n",
      "25/05/06 17:29:48 INFO BlockManagerInfo: Removed broadcast_22_piece0 on 172.21.73.132:40669 in memory (size: 37.4 KiB, free: 2.2 GiB)\n",
      "25/05/06 17:29:48 INFO SparkContext: Starting job: showString at NativeMethodAccessorImpl.java:0\n",
      "25/05/06 17:29:48 INFO DAGScheduler: Got job 22 (showString at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/05/06 17:29:48 INFO DAGScheduler: Final stage: ResultStage 30 (showString at NativeMethodAccessorImpl.java:0)\n",
      "25/05/06 17:29:48 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/05/06 17:29:48 INFO DAGScheduler: Missing parents: List()\n",
      "25/05/06 17:29:48 INFO DAGScheduler: Submitting ResultStage 30 (MapPartitionsRDD[74] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/05/06 17:29:48 INFO BlockManagerInfo: Removed broadcast_20_piece0 on 172.21.73.132:40669 in memory (size: 32.7 KiB, free: 2.2 GiB)\n",
      "25/05/06 17:29:48 INFO MemoryStore: Block broadcast_25 stored as values in memory (estimated size 77.5 KiB, free 2.2 GiB)\n",
      "25/05/06 17:29:48 INFO MemoryStore: Block broadcast_25_piece0 stored as bytes in memory (estimated size 29.9 KiB, free 2.2 GiB)\n",
      "25/05/06 17:29:48 INFO BlockManagerInfo: Added broadcast_25_piece0 in memory on 172.21.73.132:40669 (size: 29.9 KiB, free: 2.2 GiB)\n",
      "25/05/06 17:29:48 INFO SparkContext: Created broadcast 25 from broadcast at DAGScheduler.scala:1585\n",
      "25/05/06 17:29:48 INFO BlockManagerInfo: Removed broadcast_21_piece0 on 172.21.73.132:40669 in memory (size: 2.8 KiB, free: 2.2 GiB)\n",
      "25/05/06 17:29:48 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 30 (MapPartitionsRDD[74] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/05/06 17:29:48 INFO TaskSchedulerImpl: Adding task set 30.0 with 1 tasks resource profile 0\n",
      "25/05/06 17:29:48 INFO TaskSetManager: Starting task 0.0 in stage 30.0 (TID 131) (172.21.73.132, executor driver, partition 0, PROCESS_LOCAL, 9764 bytes) \n",
      "25/05/06 17:29:48 INFO Executor: Running task 0.0 in stage 30.0 (TID 131)\n",
      "25/05/06 17:29:48 INFO BlockManagerInfo: Removed broadcast_19_piece0 on 172.21.73.132:40669 in memory (size: 22.1 KiB, free: 2.2 GiB)\n",
      "25/05/06 17:29:48 INFO BlockManagerInfo: Removed broadcast_18_piece0 on 172.21.73.132:40669 in memory (size: 17.1 KiB, free: 2.2 GiB)\n",
      "25/05/06 17:29:48 INFO BlockManagerInfo: Removed broadcast_23_piece0 on 172.21.73.132:40669 in memory (size: 36.0 KiB, free: 2.2 GiB)\n",
      "25/05/06 17:29:48 INFO BlockManagerInfo: Removed broadcast_24_piece0 on 172.21.73.132:40669 in memory (size: 8.6 KiB, free: 2.2 GiB)\n",
      "25/05/06 17:29:48 INFO CodeGenerator: Code generated in 21.578133 ms\n",
      "25/05/06 17:29:48 INFO CodeGenerator: Code generated in 6.95321 ms\n",
      "25/05/06 17:29:48 INFO Executor: Finished task 0.0 in stage 30.0 (TID 131). 2808 bytes result sent to driver\n",
      "25/05/06 17:29:48 INFO TaskSetManager: Finished task 0.0 in stage 30.0 (TID 131) in 119 ms on 172.21.73.132 (executor driver) (1/1)\n",
      "25/05/06 17:29:48 INFO TaskSchedulerImpl: Removed TaskSet 30.0, whose tasks have all completed, from pool \n",
      "25/05/06 17:29:48 INFO DAGScheduler: ResultStage 30 (showString at NativeMethodAccessorImpl.java:0) finished in 0.128 s\n",
      "25/05/06 17:29:48 INFO DAGScheduler: Job 22 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/05/06 17:29:48 INFO TaskSchedulerImpl: Killing all running tasks in stage 30: Stage finished\n",
      "25/05/06 17:29:48 INFO DAGScheduler: Job 22 finished: showString at NativeMethodAccessorImpl.java:0, took 0.131807 s\n",
      "25/05/06 17:29:48 INFO CodeGenerator: Code generated in 8.577607 ms\n"
     ]
    }
   ],
   "source": [
    "predictions_2018.printSchema()\n",
    "predictions_2018.select(\"features\", target_column, \"rawPrediction\", \"probability\", \"prediction\").show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39e0b5ff",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "218a2224",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "13907fcb",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5794b4b7",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c2b18a97",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
